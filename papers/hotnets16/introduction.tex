\section{Introduction}
Modern networks, including data center, campus, 
and wide area networks, 
must satisfy increasingly complex security, 
availability, and performance objectives to 
meet the demands of multitude of users and applications. 
Many common network management systems provide support
for reachability (or lack thereof for access control),
forms of traffic engineering for optimizing performance
and fairness, service chaining through ``middleboxes'' and
other policies like isolation for performance and privacy reasons.
Thus, a high degree of programmability is required in networks
for enforcing the aforementioned policies.

The software-defined networking (SDN) architecture was introduced
for creating programmable networks, where a centralized controller
manages the forwarding rules in the switches based on the policies.
SDN enables operators to enforce various policies in their networks 
with ease. 
However, the controller is responsible for updating the forwarding
rules during a failure scenario, thus creating a central point of 
failure in the network. One of the design principles
for legacy control planes running distributed networking protocols 
(e.g., OSPF, BGP, and MPLS) was to avoid central points of failure.
Legacy control planes are scalable, robust, maintain a smaller amount 
of state in switches compared to SDNs, 
and split the computational work over all of the switches. 
Importantly, there is no central point of failure and the switches
respond locally to failures and converge to a consistent state. 

The major shortcoming of using distributed control planes is that
they are very difficult to reason about and program to cater to 
complex policies required by operators. To reduce configuration 
errors, operators use parameterized templates for common
tasks, but these approaches do not eliminate the manual effort
required to decompose the network-wide policy into individual device
behavior or help enforce custom policy behavior. While SDNs 
were introduced to ameliorate the programmability of legacy control planes, 
for various reasons, 
SDN deployment in the industry has been slow, and many 
networks still rely on these ``tried-and-tested" distributed 
control planes. 

% \begin{figure}
% 	\centering
% 	\includegraphics[width=\columnwidth]{}
% 	\caption{Architecture}
% 	\label{fig:architecture}
% \end{figure}

\missingfigure[figwidth=\columnwidth]{Architecture}

In this paper, we introduce an architecture which marries the centralized
policy control afforded by SDN and the advantages of using
distributed control planes. 
By carefully engineering the parameters determining the
behavior of a distributed control plane by solving a set
of linear equations, we can synthesize control planes which
enforce a diverse set of policies. 
Since failure handling is 
performed in a distributed manner, the behavior of the control
plane under failure scenarios is incorporated in the synthesis. 
This systematic approach overcomes the shortcoming of unwanted
behaviors of the control planes risen by manual error-prone
practices. This
formal approach to synthesis of control planes also eliminates the need of 
verification using tools like batfish, anteater, and arc. 

The major components of synthesizing distributed control planes from 
policies are three-fold: specifying input policies, the synthesis 
algorithm, and the output configurations. The architecture has to able to
support a wide array of policies common in network management like service
chaining, traffic engineering and isolation. Moreover, the operator 
has to specify policies affecting control plane behaviors under failure 
scenarios for synthesizing resilient control planes. 

The space of output network configurations to enforce a set of policies
can be large, each output can contain different compositions of 
network protocols (e.g., only BGP, only OSPF or a hybrid) and
mechanisms used (e.g., number of route filters, access control lists
or protocol-specific variables) which can complicate the synthesis of
network configurations from
input policies, or tie the synthesis to a particular type of configuration. 
Instead, we borrow a page from programming languages
research and
synthesize an Abstract Representation for Control planes (ARC) from 
the data plane; the ARC can then be translated to actual device configurations.
The ARC uses the notion that most routing protocols in use 
today employ a cost-based path selection algorithm, thus a graph with
weights on the links can be used to abstractly represent the control plane such that 
the path between two points taken in the network would be 
the shortest weight path in the graph. 
The ARC effectively decouples the policy component with the 
actual network implementation, thus, the networking infrastructure could be
transitioned to different protocols without affecting the policy 
control of the architecture. From the ARC, we can construct different
drivers for translation to actual device configurations; these drivers
can leverage inferences from healthy network practices in 
real-life networks~\cite{mpa-imc15} to produce ideal network configurations.

Contributions. \kausik{Fill later}


% We further augment this architecture with two important features: 
% a ``Default Off'' behavior and synthesizing ARCs to
% accomodate incremental policy changes. When a packet not matching any headers
% in the FIB (Forwarding Information Base) is encountered, this packet 
% should ideally be sent to the 
% policy controller for action. However, traditional control planes lack this
% ``Default Off'' behaviour, which is paramount to prevent policy violations with
% performance and security impacts. Thus, we propose changes to switch hardware/firmware(?) 
% to support this functionality of sending packets with no matching entries to the 
% policy controller (similar to how SDNs act with new packets).
% When we encounter a new packet class, operator assistance is required to create policies 
% for this class. Furthermore, operators frequently add or modify policies in practice. 
% However, every time there is a policy change, it is expensive to resynthesize the device
% configurations and deploy them in the network. Thus, 
% operators requires a mechanism to modify the 
% existing configuration with minimal changes to enforce the new policies.

% --- Network operators can design network management systems
% providing different functionalities like service chaining~\cite{simple} or traffic
% engineering~\cite{} oblivious of the underlying networking protocol.

% --- To achieve this