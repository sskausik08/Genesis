\section{Evaluation}
 \label{sec:evaluation}
 
 We implemented a prototype of \name in Python. Our prototype uses the
 LP solver Gurobi~\cite{gurobi} for synthesis of OSPF configurations.
 For a given workload, \name outputs Quagga configurations~\cite{quagga}.  In this section, we evaluate \Name using
%\loris{really don't like the word realistic}
enterprise-scale data
center fat-tree topologies~\cite{fattree} of different 
sizes. 
Specifically, we ask:
\begin{itemize}
	\item How does \name perform end-to-end going from policies to
	configurations in conjunction with \genesis for a single domain? 
	How does the synthesis time vary for different workloads and number 
	of paths? (\secref{sec:ospfeval})
	
	\item How well does \name's configuration 
	synthesis fare in terms of resilience metrics? (\secref{sec:reseval})
	
	\item How well does \name's stochastic domain assignment 
	search work in optimizing configuration overhead
	and minimizing the static routes used? (\secref{sec:mcmceval})
\end{itemize}
All experiments were conducted using a
32-core Intel-Xeon 2.40GHz CPU machine and
128GB of RAM.

\subsection{Single Domain End-to-end Performance with Genesis}\label{sec:ospfeval}

%\setlength{\intextsep}{0pt}%
%\setlength{\columnsep}{0pt}%
\begin{wrapfigure}[12]{r}{0.6\columnwidth}
	\includegraphics[width=0.58\columnwidth]{figures/ospfisolation.eps}
	%	\subfloat[Number of Route Filters]
	%	{\includegraphics[width=0.33\columnwidth]{figures/ospfRF.eps}}
	%	\subfloat[Endpoint Resilience]
	%	{\includegraphics[width=0.32\columnwidth]{figures/ospfAvgRes.eps}}
	\vspace{-8pt}
	\compactcaption{\label{fig:ospfisolation}
		End-to-end synthesis time for isolation workloads over the range of destinations and different tenant sizes.}
\end{wrapfigure}

We benchmark the end-to-end performance of Genesis and Zeppelin
for a single OSPF domain configuration  synthesis on a fat-tree 
topology comprising of 45 routers. We evaluate the performance
of the path-compliance synthesis algorithm for tenant 
isolation workloads, and single-path waypoint-compliance 
synthesis for varying number of waypoint policies.
The topology size is consistent with operator preferences to restrict
the size of a domain to under 50 routers (OSPF does not scale
well as domain size increases).

\noindent\textbf{Path-Compliance.}~~~
To benchmark the path-compliance 
configuration synthesis (referred to as 1-PC) 
for a single domain, we 
model a multi-tenant 45 router topology with 
isolation in tenants. For each 
workload, we have $n$ tenant groups, 
each group comprising of $g$ destinations. 
The x-axis shows the number of paths $n * g$. 
Within a tenant group, we add a policy to ensure
each path with randomly generated endpoints 
is isolated from other paths of the same tenant group 
(to prevent interference by other traffic belonging to
same tenant).  

\Cref{fig:ospfisolation} 
shows the end-to-end configuration synthesis time from policies 
for increasing number of tenants and different group sizes ($g =
\{10,20\}$). As the number of tenants increase, time to 
synthesize the data plane increases linearly as we only 
consider isolation policies within tenants, not amongst paths 
of different tenants. Meanwhile, time taken to synthesize 
OSPF configurations increases exponentially with increasing 
number of tenants. This is because the complexity of computing 
the unsat-core is exponential; with increasing number of 
paths, we need to add more static routes for 
path-compliance (each iteration of the unsat-core learning
procedure adds one static route). Using our two-phased approach,
we are able to synthesize configurations for 4 tenants, each with
20 destinations in 77 seconds, where Genesis takes 35 seconds and
Zeppelin takes 42 seconds on average. 


\begin{wrapfigure}{Hr}{0.6\columnwidth}
	\begin{center}
		\includegraphics[width=0.6\columnwidth]{figures/ospfwaypoint.eps}
		%	\subfloat[Number of Route Filters]
		%	{\includegraphics[width=0.33\columnwidth]{figures/ospfRF.eps}}
		%	\subfloat[Endpoint Resilience]
		%	{\includegraphics[width=0.32\columnwidth]{figures/ospfAvgRes.eps}}
		\compactcaption{\label{fig:ospfwaypoint}
		End-to-end synthesis time (log (s)) for waypoint policy workloads for varying 
		number of paths and different number of waypoint sets.}
	\end{center} 
\end{wrapfigure}

\noindent\textbf{Waypoint Compliance.}~~~
We benchmark the end-to-end synthesis time for waypoint policy 
workloads for varying number of destination IP addresses and 
waypoint sets. In this setup, we randomly pick three routers
and assign them to a waypoint set, and vary the number of 
unique waypoint sets in the topology (2, 5, 10). Each set can be 
considered as a class of replicated middleboxes (for e.g., firewall).
For each destination, we map it to one of the waypoint sets as a
Genesis waypoint policy. Genesis generates a single path for each 
destination, and \name synthesizes waypoint-compliant OSPF 
configurations ((referred to as 1-WC) ) 
using the waypoint policies and paths generated by Genesis.  

\Cref{fig:ospfwaypoint} shows the end-to-end synthesis
 time (log scale) for varying number of destinations paths and the 
 number of unique waypoint sets. Since, synthesizing paths
 for waypoint policies can be synthesized independently,
Genesis synthesis time is $<20s$. \name's 1-WC synthesis 
time increases as we increase the number of waypoint sets,
as we need to add constraints for $D_s^t(\waypt)$ 
for each set, and thus, computing the unsat-core in each
iteration to find a static route is more expensive with increasing
number of sets. For 5 waypoint sets, \name takes 300 
seconds on average to synthesize waypoint-compliant configurations. 


%\begin{table}{l}{8em}
%	\begin{footnotesize}
%		\begin{center}
%			\begin{tabular}{P{10em}| P{4em} | P{4em} | P{4em} | P{4em}}
%				Synthesis Type & Number of Packet Classes & Avg. Synthesis time (s) & Avg. Number of Resilient Classes & Ratio of Static Routes \\
%				\hline
%				1-Resilient Waypoint & 10 & 122.16 & 7.13 & 7/100\\
%				Waypoint & 10 & 7.85 & 0.3 & 7/100\\
%				1-Resilient Waypoint & 20 & 122.16 & 7.1 & 7/100\\
%				Waypoint & 20 & 7.85 & 0.3 & 7/100\\
%				1-Resilient Waypoint & 40 & 122.16 & 7.1 & 7/100\\
%				Waypoint & 40 & 7.85 & 0.3 & 7/100\\
%			\end{tabular}
%		\end{center}
%		\compactcaption{Average synthesis time per class for waypoint policies with increasing number of waypoints. } \label{tab:waypointeval} 
%	\end{footnotesize}
%\end{table} 

\subsection{Resilience Performance of Intra-domain Synthesis} \label{sec:mcmceval}
We now evaluate the resilience of configurations 
of \name's intra-domain synthesis. For this, 
we consider the path-compliance isolation workload 
in \secref{sec:ospfeval} and benchmark the connectivity-
resilience obtained over a baseline path-compliance 
configuration 
where we use static routes for all links in the path and 
assign OSPF edge weights randomly.

\noindent\textbf{Resilient Waypoint Compliance.}~~~
We now evaluate the resilience obtained by \name's
resilient waypoint-compliance synthesis (referred to as 2-WC)
described in \secref{sec:waypointres}
for the waypoint policy workload with 5 unique waypoint sets 
over two cases: (1) 1-PC: \name uses a single waypoint path
synthesized by Genesis for each policy 
and synthesizes a path-compliant configuration, and 
(2) 1-WC: \name sythesizes a waypoint-compliant configuration
using a single waypoint-path from Genesis. For these 
experiments, for the same policy input, we run \genesis + \name 
3 times with a random seed, and report the most-resilient configuration.

\Cref{fig:ospf} shows the scatter plot of the policy-resilience 
score obtained by 2-WC over 1-PC and 1-WC over 100 runs
for varying number of destinations (10, 20 and 40). 
We can observe that 2-WC is able to synthesize highly
resilient configurations (score > 0.9) over 1-PC (
average score is 0.5) and 1-WC (average score is 0.6). 
1-WC is able to provide higher policy-resilience than
1-PC because

\begin{figure*}
	\centering
	\subfloat[2-WC vs. 1-PC]
	{\includegraphics[width=0.49\columnwidth]{figures/ospfresilience2.eps}}
	\subfloat[2-WC vs. 1-WC]
	{\includegraphics[width=0.5\columnwidth]{figures/ospfresilience.eps}}
	%	\subfloat[Number of Route Filters]
	%	{\includegraphics[width=0.33\columnwidth]{figures/ospfRF.eps}}
	%	\subfloat[Endpoint Resilience]
	%	{\includegraphics[width=0.32\columnwidth]{figures/ospfAvgRes.eps}}
	\compactcaption{\label{fig:ospfres}
		OSPF Synthesis evaluation}
\end{figure*}
%\begin{figure*}
%	\centering
%	
%	%	\subfloat[Number of Route Filters]
%	%	{\includegraphics[width=0.33\columnwidth]{figures/ospfRF.eps}}
%	%	\subfloat[Endpoint Resilience]
%	%	{\includegraphics[width=0.32\columnwidth]{figures/ospfAvgRes.eps}}
%	\compactcaption{\label{fig:ospfres}
%		OSPF Synthesis evaluation}
%\end{figure*}


\subsection{Dynamic Domain Assignment Performance} \label{sec:mcmceval}
\begin{wrapfigure}{r}{0.35\columnwidth}
	\includegraphics[width=0.33\columnwidth]{figures/ratioMCMC.eps}
	\compactcaption{\label{fig:mcmceval}
		MCMC Evaluation for varying number of paths.}
\end{wrapfigure}

In this section, we demonstrate the advantages of using MCMC sampling
to find a domain assignment which can  
reduce both configuration overhead and number of static routes, 
leading to higher connectivity-resilience. 
For these experiments,
we consider a
80 router fat-tree topology. 
We run the MCMC sampling for 600s
(iterations $>$ 100,000), 
and the tunable parameter $\alpha$ assigning
priority for optimizing configuration 
overhead or static routes is set
to 1. For the input, we generate 
random $n$ paths for $n/4$
destination IPs, with path
 length chosen at random from $[3,10]$. 
We vary $n$ from 200 to 1,000.
We
split the network into 5 OSPF domains 
each with size in range $[10,40]$. We
conduct these experiments 20 times each, 
and report the average and
standard deviation of the metrics.

%\noindent\textbf{Configuration Overhead.}~~~
%\Cref{fig:mcmceval}(a) shows the configuration overhead $bc$ 
%incurred by \name for synthesizing inter-domain configurations.
%For the Fat-8 topology which has greater path diversity, we incur more 
%overhead to ensure path-compliance than the Ion topology. 
%The Best traces indicate the best domain assignment found by MCMC, while 
%Worst traces indicates the domain assignment with greatest 
%configuration cost. 
%This illustrates
%the effectiveness of MCMC sampling in finding configurations with lower overhead on average.
%
%\noindent\textbf{Endpoint Resilience.}~~~ \Cref{fig:mcmceval}(b) shows
%the loss in total endpoint resilience with varying number of paths
%(the count of backup paths for endpoints filtered by the
%configurations). For the Worst traces, we store the worst domain
%assignment in terms of route-filter cost and synthesize the OSPF
%configurations for each domain and calculate the total resilience
%loss. We observe that the MCMC sampling can find domain assignments
%with lower resilience loss than the worst case. This also illustrates
%the effectiveness of the route-filter estimate we use for the cost
%function: a reduction in cost results in greater resilience.
For each MCMC run, we find the best configuration
which optimizes the user-defined quantity based on 
configuration overhead and number of static routes. 
We also store the worst configuration in terms of either
configuration overhead $bc$ and the static route cost 
$sc$. In \Cref{fig:mcmceval}, we plot the average Conf. ratio 
(best configuration overhead/worst configuration overhead), 
the average static route (SR) ratio 
(best / worst number of static routes) and the average 
connectivity-resilence ratio (best/worst connectivity-resilience obtained corresponding to the 
static route cost)
for varying number of paths. 
In smaller workloads, we obtain a 0.5$\times$ reduction, 
but incur an increase in number of static routes. For 
larger workloads, the MCMC sampling is able to
reduce both the static routes by $0.35\times$ and 
BGP configuration overhead by $0.3\times$. 
%The tunable parameter $\alpha$ 
%can be used to assign different priorities to these objectives to get
%different trends.
\kausik{Talk about connectivity-resilience} 



